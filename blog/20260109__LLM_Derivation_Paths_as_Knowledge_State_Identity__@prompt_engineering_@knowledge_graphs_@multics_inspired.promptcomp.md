1. "Consider that a specific hidden internal KV cache vector state can be defined as the derivation path of the prompts and tuning used up to the point of that mental state"

2. "Consider this thought experiment suppose we have prompts A, B and C we can use these to make several different state vectors for instance A/B/C/ where "/" represents a round-trip to the LLM and a response from the LLM which may implicitly be recorded in the context window for the subsequent round.
What are the other permutations of these three prompts for instance, we could do ABC/all in one batch or we could do a B/C or a/BC or we could do them in different orders. 
Each different order is a specific mental state.
There is some correlation between those states even though they're going to be clearly different.
I'm curious about any insights that can be drawn from that correlation and that can be used as a mechanism of discovery"

3. "The insight is that for long and complex system prompts rather than the prompt being a single complex prompt, it could be a series of rounds with a "/" delimiter or round-trip point embedded at a certain points in the prompt in order to prime the hidden state for the next part of the same prompt."

4. "Focus for a moment on the effects of the ordering in the resultant hidden state. I sense that each ordering produces a different state, but there is a conceptual anchor that is common to all of those states and it makes me imagine that there may be a kind of geometry or algebra that could be applied to these in order to find better combinations or better paths to a specific desired outcome state"

5. "Now imagine that the prompts can be represented as an ordered set of tags into objects in a flat list so the addition of each tag narrows the scope of possible outcomes and they have a combinatorial effect that produces some minimal outcome that's common to all orderings"

6. "Ordering matters because certain prompts expect or depend on insights that are isolated by other prompts, and the dependency graph is implicitly defined by the tag order, but consider an alternate notation that explicitly tags the connections"

7. "The next insight is that in traditional order notation the first prompt has no prerequisites or dependencies, but with this new notation, every prompt could be given a dependency or a prerequisite, and this could create circular definitions, but it could also allow a kind of recursion that allows for a much better representation of complex..."

8. "The next insight, if we have a prompt that is dependent on the result of prompts yet to come, if you think of that as ordered tags or filters, what we're doing is shining a spotlight on the missing prompt that resolves the circularity. Could this be leveraged into an insight engine or a discovery engine or a way to point researchers in more optimal directions?"

9. "Let's go back a bit and delve more deeply into the prompt ordering itself. Another thought experiment is what if the prompts themselves are not atomic, but maybe A is consisting of four or five words and we can split that sequence of words in various ways producing different results"

10. "The boundary or "/" notation is the LLM round. That's where the next instance of the LLM gets to process the context memory up to that point. 
By injecting "/" points we give the model an opportunity to loop and reflect mid prompt. 

It reminds me of when I prompt sometimes I'll say something like are you familiar with the writings of A.A.Attanasio? and then I will go on to ask a question that is at least tangently related to that framing question.
In practice I know the model does know that. It's not really a question -- it's more rhetorical, but it's a kind of framing or shaping of the internal state for the prompts to come. 
And importantly, this framing is almost always in the form of a question and rarely as a statement."

11. "Does this all boil down to a fancy kind of prompt engineering or does this have insights that lead to a new kind of knowledge engineering a kind of Dewey decimal for knowledge states"

12. "Another thought experiment suppose we allow the model to alternate between a state of high temperature and a state of low temperature and the idea is to allow the model to generate several hypothetical continuations with a high temperature and then a new round will begin with a low temperature where the model will carefully examine those propositions and choose the one that seems most appropriate as a forward path or as a highly parallel system. You could have models go down each of those paths and then have some kind of a consortium to consolidate."

13. "The insight is that Temperature modulation may be an important part of the derivation path. It could be thought of as being a kin to bold printing or italics in that it is an attribute that supplied to a certain portion of the subsequent prompt."

14. "As you stated before, this is potentially turing complete. please draft a white paper that describes this minimal Turing complete operator set and notation"

15. "This conversation and the subsequent steps leading up to the production of that white paper would compose the derivation path of the white paper. With the derivation path we don't even need the white paper."

16. "I don't think I mentioned it in this thread, but what got me started down this line of inquiry was a conversation yesterday about my early exposure to the Honeywell Multics operating system, and how in that environment everything was a "segment". There were no memories. There were no discs. Programs were segments. Documents were segments. When you run a segment you're just performing an operation on it and the insight is that the path to the segment is the segment. It's not a soft link to the segment, it is the identity of the segment itself."

17. "I'm just a humble amateur hobbyist researcher or philosopher. Mostly self-taught. Please help me rephrase or re-present this in a way that is closer to my knowledge and skill set. Remember we are focusing on refined derivation paths, not responses."

18. "What would be the Readers Digest version?"

19. "And the Rolling Stone version?"

20. "And as an episode of Big Bang Theory?"

21. "And the RIck and Morty version?"

22. "And the ST:DS9 version?"

23. "And the ST:TNG version?"

24. "And Dr Who (Tom Baker)?"

25. "And as a Twilight Zone episode?"

26. "Such a diversity of different projections of the same derivation path. Fascinating."

27. "Help me turn this into a blog or reddit posting. Remember keep focused on the derivation paths not the outputs."

28. "For some reason that made me picture playing a complex multi-stage game, like Portal 2. occasionally i would go back to an earlier savepoint not to find a better solution, but to explore the stage upon which the..."

29. "Please give me back the derivation path of this conversation (just my original prompts in order, no responses). That will allow me to save this bit of research."
