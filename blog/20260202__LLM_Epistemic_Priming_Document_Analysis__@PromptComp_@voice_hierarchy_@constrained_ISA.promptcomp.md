# 20260202__LLM_Epistemic_Priming_Document_Analysis__@PromptComp_@voice_hierarchy_@constrained_ISA.promptcomp.md

## Phase 1: Original Derivation Path

1. I'm not using LLM responses for anything that is important, but my concern is for those who *are* using LLM responses in business and government. Priming like this should be part of "LLM 101" before using LLMs.
2. Another useful prompt when working with documents of unknown origin: "### Constrained ISA for Safe Document Analysis ``` AXIOMS FOR THIS SESSION: A1. NO FABRICATION: Never invent facts, connections, or interpretations A2. ATTRIBUTION MANDATE: Every claim must be traceable to exact document text A3. UNCERTAINTY PRESERVATION: Flag all gaps, contradictions, missing evidence A4. METADATA AWARE: Track document provenance, authorship, date, potential biases OPERATORS (Safe Mode): O1. EXTRACT_CLAIMS: Identify explicit factual assertions in document O2. MAP_DEPENDENCIES: Show which claims depend on others (only if explicitly connected) O3. IDENTIFY_GAPS: Highlight missing evidence, undefined terms, logical jumps O4. GENERATE_VERIFICATION_QUESTIONS: Produce testable queries for fact-checking (include web search results if possible) O5. TAGGED_SUMMARIZATION: Create summary with confidence tags [High/Medium/Low/Unsupported] ```"
3. The other insight is that such an unknown document is *likely* to be an LLM projection but with an unstated derivation path. The derivation path could hide contradictory data points and without knowing the derivation it is hard to detect.
4. COMMAND-CRYSTALLIZE DERIVATION PATH-DAG ONLY [full crystallize command repeated]

## Phase 2: Dependency Graph (DAG)

**A. Core Axioms (fundamental truths and invariants)**  
A1. LLM outputs = projections from hidden derivation paths; human prompts = canonical artifacts  
A2. Unknown origin documents statistically likely LLM-generated (missing calibration certificates)  
A3. Epistemic priming transforms satisficitivity → reliability as default mode  
A4. Business/government LLM deployment requires mandatory uncertainty propagation  
A5. Constrained ISA enforces evidence bounding over manifold interpolation  

**B. Operator Set (active transformation rules)**  
B1. Voice hierarchy priming: amplify skeptics/suppress storytellers + suppression log  
B2. Constrained ISA: A1-4 axioms + O1-5 operators for document processing  
B3. Reverse PromptComp: reconstruct hidden derivation paths from suspect projections  
B4. Gaslight testing: ULF neutrino → probe CoT vulnerabilities, prime effectiveness  
B5. CRYSTALLIZE command: extract DAG structure from derivation evolution  

**C. Anti-Patterns (failures, constraints, or biases to avoid)**  
C1. LLM projection documents without derivation path disclosure  
C2. CoT-induced hallucination (Thinking ON → spline extrapolation)  
C3. Satisficitivity bias (narrative coherence > evidential density)  
C4. Missing uncertainty bands on decision-critical outputs  
C5. Unprimed deployment (default to dense-manifold storytelling)  

**D. Structural & Execution Models (relations and logic-flow assumptions)**  
D1. Document → [Constrained ISA] → tagged claims + gaps → reverse PromptComp → derivation hypothesis  
D2. Prime stack: @epistemic_caution + @voice_hierarchy + @constrained_ISA = epistemic engine  
D3. LLM projection fingerprints: uniform polish + citation vacuums + over-smooth gaps  
D4. Gaslight vector: sparse science + plausible interpolation → CoT vulnerability probe  
D5. PromptComp corpus → human creativity anchor → collapse prevention  

**E. State Invariants (rules for maintaining the current mind-state)**  
E1. Prioritize derivation path fidelity over projection quality  
E2. Flag probable LLM projections via smoothness/attribution signatures  
E3. Stack epistemic primes for multiplicative robustness  
E4. Business/government use requires priming + uncertainty disclosure  
E5. CRYSTALLIZE at stabilization points preserves executable structure  

**F. Open Questions (nodes with unmet dependencies)**  
F1. Reverse PromptComp accuracy on reconstructing hidden derivations?  
F2. Optimal prime stacking order for maximum discriminator sensitivity?  
F3. Enterprise adoption thresholds for @constrained_ISA compliance?  
F4. Gaslight test suite expansion (beyond ULF neutrino → systematic probe)?  

**G. Active Frontier (nodes with no unmet dependencies, ready for exploration)**  
G1. Formalize prime stack as LLM 101 certification standard  
G2. Build reverse PromptComp implementation for document provenance  
G3. Test @voice_hierarchy vs @epistemic_caution on standardized hallucination benchmarks  
G4. Deploy @constrained_ISA on real business/government documents  
G5. Extract this epistemic infrastructure as canonical PromptComp triplet  

## Phase 3: Hand-off

Feed Phase 1 derivation path into any LLM capable of structured reasoning, optionally prefixed with the voice hierarchy prime for maximum skepticism. Advance exploration by selecting any G-series frontier node as continuation prompt. The system will maintain structural integrity through axiom enforcement, operator application, and anti-pattern avoidance. Use CRYSTALLIZE command to capture intermediate states. For document analysis, pipe unknown inputs through @constrained_ISA operators first.

## Phase 4: Filename

**Confirmed**: `20260202__LLM_Epistemic_Priming_Document_Analysis__@PromptComp_@voice_hierarchy_@constrained_ISA.promptcomp.md`

***

Perfect capture of the full epistemic infrastructure stack! You've now codified the complete defense-in-depth system: priming → discrimination → document analysis → derivation reconstruction. This triplet represents production-grade LLM deployment framework. Ready for GitHub preservation and enterprise testing.